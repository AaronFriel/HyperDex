.. _asynchronous:

Fault Tolerance
===============

HyperDex recovers from failures automatically.  When a node fails, HyperDex
detects the failure, removes the failed node, and automatically reintegrates
other nodes to restore the desired level of fault tolerance.  The coordinator is
made fault tolerant using the Replicant replicated state machine.  It, too, is
fault tolerant and can tolerate a configurable number of failures.

Let's see both kinds of fault tolerance in action.

Coordinator
-----------

First, let's demonstrate the coordinator's fault tolerance.  Bring up a
singly-fault tolerant coordinator using the same procedure as in previous
chapters.  Note that this single node coordinator can shutdown and restart, but
is unavailable while it is shutdown for obvious reasons.

.. sourcecode:: console

   $ replicant daemon -f -l 127.0.0.1 -p 1982
   $ hyperdex initialize-cluster -h 127.0.0.1 -p 1982

A Replicant cluster becomes more fault tolerant as more nodes are added.  A
simple majority of the nodes must remain available for Replicant to make
progress.  For example, a cluster with three nodes can fail one node at any
point in time, while a cluster with five nodes can tolerate two simultaneous
failures.

In separate terminals we can bring additional nodes online:

.. sourcecode:: console

   $ replicant daemon -f -l 127.0.0.1 -p 1983 -c 127.0.0.1 -P 1982 -D /path/to/data1
   $ replicant daemon -f -l 127.0.0.1 -p 1984 -c 127.0.0.1 -P 1982 -D /path/to/data2
   $ replicant daemon -f -l 127.0.0.1 -p 1985 -c 127.0.0.1 -P 1982 -D /path/to/data3
   $ replicant daemon -f -l 127.0.0.1 -p 1986 -c 127.0.0.1 -P 1982 -D /path/to/data4

For each new server in the cluster, Replicant will automatically transfer a copy
of the initialized state machine and replicate further operations on those nodes
as well.  In it's current form our cluster can tolerate two simultaneous
failures or planned shutdowns.  Go ahead and shutdown one or more Replicant
servers by pressing Control-C.  When restarted, they'll rejoin the cluster and
collect any state changes that occurred in their absence.

It's even possible to completely shutdown and restart a Replicant cluster by
shutting down and restarting all nodes in the cluster.

HyperDex Daemons
----------------

HyperDex daemons are organized by the coordinator.  In response to failures and
shutdowns, the coordinator automatically responds by assigning a new server to
take its place and initiating recovery procedures to ensure the new server
maintains an up-to-date view of the data.

To see this in action, we first need to start a cluster:

.. sourcecode:: pycon

   $ hyperdex daemon -f --listen=127.0.0.1 --listen-port=2012 \
                        --coordinator=127.0.0.1 --coordinator-port=1982 --data=/path/to/data1
   $ hyperdex daemon -f --listen=127.0.0.1 --listen-port=2013 \
                        --coordinator=127.0.0.1 --coordinator-port=1983 --data=/path/to/data2
   $ hyperdex daemon -f --listen=127.0.0.1 --listen-port=2014 \
                        --coordinator=127.0.0.1 --coordinator-port=1984 --data=/path/to/data3

If you look carefully, you'll see that we've asked each daemon to connect to a
different Replicant server.  Each daemon connects to the Replicant cluster in a
fault-tolerant manner.  Should the specified server fail after the daemon
connects, the daemon will simply connect to another server in the cluster.
When a failure of either a coordinator node or a daemon occurs, the Replicant
system ensures that all servers in the system will eventually be able to make
progress.

Let's see this in action by creating some data items we care deeply about and
checking what happens to our data as failures occur:

.. sourcecode:: pycon

   >>> c.add_space('''
   ... space phonebook
   ... key username
   ... attributes first, last, int phone
   ... subspace first, last, phone
   ... create 8 partitions
   ... tolerate 2 failures
   ... ''')
   >>> c.put('phonebook', 'jsmith1', {'phone': 6075551024})
   True
   >>> c.get('phonebook', 'jsmith1')
   {'first': 'John', 'last': 'Smith', 'phone': 6075551024}

So now we have a data item we deeply care about. We certainly would not want our
NoSQL store to lose this data item because of a failure. Let's create a failure
by killing one of the three hyperdaemon processes we started in the setup phase
of the tutorial. Feel free to use "kill -9", there is no requirement that the
nodes shut down in an orderly fashion.  HyperDex is designed to handle a limited
number of crash failures.

.. sourcecode:: pycon

   >>> # kill a node at random
   >>> c.get('phonebook', 'jsmith1')
   {'first': 'John', 'last': 'Smith', 'phone': 6075551024}
   >>> c.put('phonebook', 'jsmith1', {'phone': 6075551025})
   True
   >>> c.get('phonebook', 'jsmith1')
   {'first': 'John', 'last': 'Smith', 'phone': 6075551025}
   >>> c.put('phonebook', 'jsmith1', {'phone': 6075551026})
   True

So, our data is alive and well. Not only that, but the subspace is continuing to
operate as normal and handling updates at its usual rate.

Let's kill one more server.

.. sourcecode:: pycon

   >>> # kill a node at random
   >>> c.get('phonebook', 'jsmith1')
   Traceback (most recent call last):
   File "<stdin>", line 1, in <module>
   File "hyperclient.pyx", line 473, in hyperclient.Client.put ...
   File "hyperclient.pyx", line 499, in hyperclient.Client.async_put ...
   File "hyperclient.pyx", line 255, in hyperclient.DeferredInsert.__cinit__ ...
   hyperclient.HyperClientException: Connection Failure
   >>> c.get('phonebook', 'jsmith1')
   {'first': 'John', 'last': 'Smith', 'phone': 6075551026}

Note that the HyperDex API exposes some failures to the clients at the moment,
so a client may have to catch HyperClientException and retry the operation.  The
HyperDex library does not resubmit failed operations on behalf of clients.
In this example, behind the scenes, there were two node failures in the
triply-replicated space. Each failure was detected, the space was repaired by
cleaving out the failed node, and normal operations resumed without data loss.

Fault Tolerance Thresholds
--------------------------

HyperDex and Replicant each tolerate a configurable number of failures before
the system fails completely.  For sake of convenience, we refer to this
threshold as ``f``, the number of servers that may fail at any one time.  For a
given value of ``f``, HyperDex needs ``f + 1`` replicas and Replicant needs ``2f
+ 1``.  Both HyperDex and Replicant can tolerate up to ``f`` total failures
where the server crashes or goes up in a blazing fire.  The behavior in this
case is to simply provision another node to replace the failed node.  As an
added bonus, both systems are able to tolerate more than ``f`` failures so long
as enough nodes rejoin the cluster to bring the number of failures back under
the failure threshold.

Shutting Down and Restoring a Cluster
-------------------------------------

Completely shutting down a HyperDex cluster is simple.  First, stop all client
traffic.  Second, interrupt each daemon using SIGHUP, SIGINT, or SIGTERM.  The
daemons will shutdown properly, informing the coordinator of their state as they
shutdown.  Finally, once all daemons have exited, interrupt the coordinator
processes using SIGHUP, SIGINT, or SIGTERM.

Restoring a cluster is just the reverse process:  first, restart the Replicant
nodes, and then restart the HyperDex nodes.
